{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"JKK","text":"<p>The Vehicle Industry Research Center has been working since May 2011 in Gyor, Hungary at the Sz\u00e9chenyi University. Understanding and researching how people and vehicles cooperate is an essential skill when designing the future of traffic. We believe that fully self-driving (a.k.a. autonomous) technology can lead to safe, easy, and sustainable transportation. We are preparing for this new technology-to-come by studying and researching its fundamentals and exploring the possibilities it offers. This process helps us gain unique knowledge on the mixed field of mechatronics, robotics, and artificial intelligence. Future transportation can be safe, easy, and sustainable without compromises.</p> <p>One of our most researched topic is self-driving (a.k.a autonomous) vehicles. We believe that fully self-driving technology can lead to safe, easy and sustainable transportation. We are preparing for this new technology-to-come by studying and researching its fundamentals and exploring the possibilities it offers. This process helps us gain unique knowledge on the mixed field of mechatronics, robotics and artificial intelligence. Future transportation can be safe, easy and sustainable without compromises.</p> <p></p>"},{"location":"dataset/","title":"Dataset overview","text":""},{"location":"dataset/#jkk_dataset_01","title":"<code>JKK_DATASET_01</code>","text":"<p>This dataset consists of measurement log files (ROS 1 rosbag), pointcloud files and additional scripts to access and edit these. The data is provided for research and educational purposes.</p> <p>Details</p> <p></p>"},{"location":"dataset/#jkk_dataset_02","title":"<code>JKK_DATASET_02</code>","text":"<p>This dataset consists of measurement log files (ROS 2 mcap) with additional scripts to access and edit these. The data is provided for research and educational purposes.</p> <p>Details</p> <p></p>"},{"location":"dataset/#jkk_dataset_03","title":"<code>JKK_DATASET_03</code>","text":"<p>This dataset contains the Human-like Behavior (HLB) usecase data, in mat format.</p> <p>Details</p> <p></p>"},{"location":"dataset/jkk_dataset_01/","title":"jkk_dataset_01","text":"<p>The log data is in .bag format, the standard logging format for ROS. To simply view and play the data Foxglove Studio is the easiest solution. It works on Windows, Linux and Mac. Another popular option is MATLAB. The data can be imported, viewed and edited in MATLAB. If you are familiar with ROS C++ or python can be a good option too. Python also offers possibilities top open the rosbags without ROS, similarly to MATLAB. The postprocessed 3D pointcloud data is in .pcd (Point Cloud Data) file format, it is a common format used inside Point Cloud Library (PCL). Also pcd can be imported easily to MATLAB / python. One of our most researched topic is self-driving (a.k.a autonomous) vehicles. We believe that fully self-driving technology can lead to safe, easy and sustainable transportation. We are preparing for this new technology-to-come by studying and researching its fundamentals and exploring the possibilities it offers. This process helps us gain unique knowledge on the mixed field of mechatronics, robotics and artificial intelligence. Future transportation can be safe, easy and sustainable without compromises.</p>"},{"location":"dataset/jkk_dataset_01/#leaf-2022-03-18-gyorbag","title":"<code>leaf-2022-03-18-gyor.bag</code>","text":"<p>Size: <code>2.12 GB</code> </p> <p>Go to details</p> <p>Download from here or get it with <code>wget</code>:</p> <pre><code>wget https://laesze-my.sharepoint.com/:u:/g/personal/herno_o365_sze_hu/EVlk6YgDtj9BrzIE8djt-rwBZ47q9NwcbgxU_zOuBji9IA?download=1 -O leaf-2022-03-18-gyor.bag\n</code></pre> <p></p>"},{"location":"dataset/jkk_dataset_01/#leaf-2021-04-23-campusbag","title":"<code>leaf-2021-04-23-campus.bag</code>","text":"<p>Size: <code>3.37 GB</code> </p> <p>Go to details</p> <p>Download from here or get it with <code>wget</code>:</p> <pre><code>wget https://laesze-my.sharepoint.com/:u:/g/personal/herno_o365_sze_hu/EYl_ahy5pgBBhNHt5ZkiBikBoy_j_x95E96rDtTsxueB_A?download=1 -O leaf-2021-04-23-campus.bag\n</code></pre> <p></p>"},{"location":"dataset/jkk_dataset_01/#leaf-2021-07-02-zala-uni-trackbag","title":"<code>leaf-2021-07-02-zala-uni-track.bag</code>","text":"<p>Size: <code>1.16 GB</code> </p> <p>Go to details</p> <p>Download from here or get it with <code>wget</code>:</p> <pre><code>wget https://laesze-my.sharepoint.com/:u:/g/personal/herno_o365_sze_hu/EaUlnq2KcQBHkCLB52nuPtQBw-FXYby23VUuwk6jmVzJBA?download=1 -O leaf-2021-07-02-zala-uni-track.bag\n</code></pre> <p></p>"},{"location":"dataset/jkk_dataset_01/#leaf-2020-06-10-campusbag","title":"<code>leaf-2020-06-10-campus.bag</code>","text":"<p>Size: <code>2.36 GB</code> </p> <p>Go to details</p> <p>Download from here or get it with <code>wget</code>:</p> <pre><code>wget https://laesze-my.sharepoint.com/:u:/g/personal/herno_o365_sze_hu/ETGGWQ0z5FxDkj3vwsjRPJEBuMwnFavgEU9aF0ol4NlwDA?download=1 -O leaf-2020-06-10-campus.bag\n</code></pre>"},{"location":"dataset/jkk_dataset_02/","title":"jkk_dataset_02","text":"<p>The log data is in .mcap format, the standard logging format for <code>ROS 2</code>. <code>MCAP</code> is an open source container file format for multimodal log data. It supports multiple channels of timestamped pre-serialized data, and is ideal for use in pub/sub or robotics applications.</p>"},{"location":"dataset/jkk_dataset_02/#getting-started","title":"Getting started","text":""},{"location":"dataset/jkk_dataset_02/#download-the-mcap-bag-files","title":"Download the <code>mcap</code> (bag) files","text":"<p>Download every MCAP as a ZIP</p> <p>Download a sample MCAP</p> <p>You can instanly view the data in Foxglove Studio (Free, online or on ay platform).</p> <p></p> <p>One of the easiest way to getting started with the dataset is to look at the notebook examples:</p> <p>Open MCAP in python notebook</p>"},{"location":"dataset/jkk_dataset_02/#dataset-description","title":"Dataset description","text":"Route Name Description Terrain <code>nissan_zala_90_country_road_1</code> road section flat - no hills <code>nissan_zala_90_country_road_2</code> longer stretches of highway, slight bends, some roundabouts hilly road <code>nissan_zala_50_sagod</code> slightly winding roads with some sharper turns 1 slight uphill <code>nissan_zala_50_zeg_1</code> mostly going in one direction, interrupted by roundabouts, continuous going flat - no hills <code>nissan_zala_50_zeg_2</code> roundabouts, bends, stationary situations (due to traffic) flat - no hills <code>nissan_zala_50_zeg_3</code> square bends with parking flat - no hills <code>nissan_zala_50_zeg_4</code> winding flat - no hills <code>nissan_zala_90_mixed</code> dynamic, city and country road mostly flat, last about 10m hilly"},{"location":"dataset/jkk_dataset_02/#usage-in-ubuntu-windows-wsl","title":"Usage in Ubuntu / Windows WSL","text":""},{"location":"dataset/jkk_dataset_02/#install-mcap","title":"Install <code>mcap</code>:","text":"<pre><code>pip install mcap\n</code></pre>"},{"location":"dataset/jkk_dataset_02/#download-dataset-eg-to-mntcbagjkkds02","title":"Download dataset, e.g. to <code>/mnt/c/bag/jkkds02/</code>:","text":"<pre><code>cd /mnt/c/bag/jkkds02/\n</code></pre> <pre><code>wget https://laesze-my.sharepoint.com/:u:/g/personal/herno_o365_sze_hu/EVofDCG_ORZJh--XTVLFsFEBOUYB1eAbHAzdTVDdf19Y9g?download=1 -O jkkds02.zip\n</code></pre> <p>Make sure you have <code>unzip</code> (<code>sudo apt-get install unzip</code>) and:</p> <pre><code>unzip jkkds02.zip\n</code></pre>"},{"location":"dataset/jkk_dataset_02/#some-images","title":"Some images","text":""},{"location":"dataset/jkk_dataset_03/","title":"jkk_dataset_03","text":"<p>This dataset contains the raw data of naturalistic driving, utilized for Human-Like Behavior studies of Automated Vehicles (HLB4AV).</p>"},{"location":"dataset/jkk_dataset_03/#jkk_dataset_03zip","title":"<code>jkk_dataset_03.zip</code>","text":"<p>Download from here or get it with <code>wget</code>:</p> <pre><code>wget https://laesze-my.sharepoint.com/personal/igneczi_gergo_ferenc_hallgato_sze_hu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Figneczi%5Fgergo%5Fferenc%5Fhallgato%5Fsze%5Fhu%2FDocuments%2FDataset%2Fjkk%5Fdataset%5F03&amp;ga=1 -O leaf-2022-03-18-gyor.bag\n</code></pre> <p>The data contains information of 17 drivers, who were selected to have relevant driving experience. Dr001-Dr003 are professional drivers who have extra driving certificate. The following table shows the details of participants:</p> Driver ID Type Age Driving Experience Driving Frequency Milage per year 001 N-P 31 10+ 3 4 002 N-P 32 10+ 3 4 003 N-P 28 10+ 3 5 004 N-P 31 10+ 4 5 005 N-P 46 10+ 4 4 006 N-P 25 6-10 2 2 007 N-P 29 10+ 3 3 008 N-P 28 3-6 2 3 009 N-P 28 1-3 1 2 010 N-P 43 10+ 4 4 011 N-P 31 10+ 3 3 012 N-P 44 10+ 3 3 013 N-P 52 10+ 4 4 014 N-P 36 10+ 4 5 015 N-P 32 10+ 3 4 021 N-P 51 10+ 4 5 023 N-P 39 10+ 2 3 <p>Explanation to notations: - P: Professional, N-P: Non-Professional  - Driving Frequency: 1: few times a year, 2: few times a month, 3: few times a week, 4: every day. - Driving Milage per year: 1: 0-1000km, 2: 1000-3000km, 3: 5000-10000km, 4: 10000-25000km, 5: more than 25000km.</p> <p>The dataset uses the following coordinate system of the vehicle:</p> <p></p> <p>Even though the data is recorded considering only two dimensional movement, the Z axis is displayed to give the right explanation of the yaw rate signal. Always, positive direction of a rotational quantity means CCW direction.</p>"},{"location":"dataset/jkk_dataset_03/#reference-platform","title":"Reference Platform","text":"<p>Data was recorded using two different vehicle platforms.</p> <p>The reference platform is a dedicated test vehicle, equipped with multiple environment sensors, also with direct access to the CAN network of the vehicle. The type of the vehicle is a Volkswagen Golf VII Variant, with a 1.4 TSI engine and 7-shift automatic gearbox.</p> <p>There are various sensor devices in this vehicle: - Genesys ADMA Gen3 (DGPS), source: ADMA 3.0 Technical Documentation, Document revision: 1.9, Date: 02/2019. Device includes the sensing of vehicle kinematic states. - Bosch Second Generation Multi-Purpose Camera (MPC2.5), including lane edge detection. - Bosch Fourth Generation Radar sensor, including object detection in ego lane and the two neighbouring lanes. - CAN data, steering torque and steering angle values.</p> <p>Reference Platform was used for measurements with Driver 1, 2 and 3 (professional drivers).</p>"},{"location":"dataset/jkk_dataset_03/#test-platform","title":"Test Platform","text":"<p>The test platform is a vehicle which is used when nonprofessional drivers are measured. The test vehicle is a Skoda Octavia MK3, with automatic gearbox. The vehicle systems were not modified, and no driver-assistance function was activated during testing. This vehicle was only equipped with the Genesys ADMA Gen3 (DGPS) device. The lane position information was reconstructed from the static lane map and the localization of the vehicle.</p>"},{"location":"dataset/jkk_dataset_03/#offline-calculations","title":"Offline Calculations","text":""},{"location":"dataset/jkk_dataset_03/#static-lane-map","title":"Static Lane Map","text":"<p>Based on the reference measurements, the lane map of the test route was created. For this, the lane position provided by the video camera was used. The lane position accuracy has +/- 2cm, while the localization has accuracy of +/- 1 cm.</p> <p></p> <p>The camera provides information of the position of the lane edges at each sample time. The following information are provided: - lane position ($d$) - lane orientation ($\\Theta$) - lane curvature ($\\kappa$). These data are then interpolated to a resolution of 5 cm travel distance through the route, using MATLAB function spline. Also, the lane edge position are transformed to the global UTM coordinates considering the pose of the vehicle. In the end, the map data contains high resolution geometry of the lane edges and also the higher level geometrical quantities (orientation and curvature).</p>"},{"location":"dataset/jkk_dataset_03/#lane-reconstruction","title":"Lane Reconstruction","text":"<p>For the test platform, the static lane map information is transferred back to the vehicle coordinate frame at each time sample. This replaced the video camera information, therefore the lane position of the test vehicle is available, with an accuracy of +/- 3 cm. Unfortunately, the dynamic information (e.g., other objects in the lane) are not available for the test platform.</p>"},{"location":"dataset/jkk_dataset_03/#dash-cam-data","title":"Dash-cam data","text":"<p>For multiple drivers' data (currently noted by \"_withTraffic\" tag in the file name) a dash cam video is available. This video is not applicable to use for e.g., computer vision algorithms, but serve more as an informal source of traffic data. However, by manual labelling the following information were added to the data:  1. Oncoming traffic time to pass: when a vehicle appears on the camera image, a timer starts to count down from a certain initial value, until the vehicle passes the test vehicle. Initial value is 2.38 seconds for small vehicles (higher preceeding speed) and 3.38 seconds for trucks (lower preceeding speed). Times were calculated based on the experiments. When there is no oncoming traffic detected, time-to-pass is set to 2.38 seconds, reflecting the fact that a vehicle may turn up in any minute. Therefore, drivers are assumed to be prepared as there would already be oncoming traffic. When a vehicle is followed, the initial value is decreased to the half for both types of vehicles. 2. Oncoming traffic type: 0: no oncoming traffic, 1: small vehicle, 2: truck and 3: convoy.</p> <p>An example of a convoy, passing the ego vehicle is shown here:</p> <p></p> <p>The same scenario on the dash cam video (snippets cut by hand):</p> <p></p> <p>Please be noted, that this information is added manually, therefore not suitable for quantitative evaluation, only qualitative!</p>"},{"location":"dataset/jkk_dataset_03/#summary-of-signals","title":"Summary of Signals","text":"Signal Name Description Unit Range Availability Source VelocityX Longitudinal velocity of the vehicle m/s 0 - 40 both platforms ADMA SteeringTorque Torque applied by the driver on the steering wheel Nm +/-3 reference platforms CAN LaneOrientation Orientation of the mid-lane in the ego frame (positive: CW, negative: CCW) rad +/- 0.1 both platforms Camera LaneEdgePositionRight Position of the lane edge of the ego lane on right side (positive: left, negative: right) m +/- 2 both platforms Camera LaneEdgePositionLeft Position of the lane edge of the ego lane on left side (positive: left, negative: right) m +/- 2 both platforms Camera LaneCurvature Curvature of the lane in the vehicle position 1/m +/- 0.005 both platforms Camera ObjectDistanceFront Distance of the vehicle in the ego lane (if no vehicle is present, value is zero) m 0-250 m reference platforms Radar ObjectVelocityFront Absolute velocity of the vehicle in the ego lane (if no vehicle is present, value is zero) m/s 0-40 reference platforms Radar ObjectAccelerationFront Absolute acceleration of the vehicle in the ego lane (if no vehicle is present, value is zero) m/s^2 +/- 10 reference platforms Radar YawRate Yawrate of the vehicle around the Z axis rad/s +/- 0.1 both platforms ADMA AccelerationX Longitudinal acceleration of the vehicle m/s^2 +/- 10 both platforms ADMA AccelerationY Longitudinal acceleration of the vehicle m/s^2 +/- 10 both platforms ADMA RoadWheelAngleFront Road-wheel-angle of the front wheels rad +/- 0.1 reference platforms CAN GPS_time Global GPS time ms - both platforms ADMA GPS_status Global GPS status enum 1: GPS, 2: RTK float, 4: RTK_course, 8: RTK_Fixed both platforms ADMA LongPos_abs Global longitudinal position of the vehicle \u00b0 - both platforms ADMA LatPos_abs Global lateral position of the vehicle \u00b0 - both platforms ADMA Relative_time Relative time stamp to the beginning of the measurement s - both platforms ADMA <p>The data is recorded in every 10 ms. Localization information is available at every 50 ms.</p>"},{"location":"dataset/jkk_dataset_03/#github-repo","title":"GitHub repo","text":"<p>The following repository contains algorithms for driver model analysis and prototypes for ADAS functions endowed with human-like features.</p> <p>github.com/gfigneczi1/hlb</p> <p>The following scripts are used for the data process:</p> <ul> <li>Lane reconstruction from reference data:     Run segmentor profile \"MapValidation\" and evaluator profile \"MapValidation\", based on this description: </li> </ul> <p>Evaluation description </p> <p>Place the raw mat files (without the map data) into the _temp folder.</p> <ul> <li>Traffic information for which dash-cam video is available:</li> </ul> <p>Traffic label procession </p> <p>In this case, you shall define the path of the corresponding xlsx file that contains the relevant traffic information.</p> <ul> <li>Standardize names for the proper storing:</li> </ul> <p>Standardize names </p> <ul> <li>Merge data for drivers where multiple short runs are available:</li> </ul> <p>Merge data </p> <p>For this script, you shall also store all unmerged data in the _temp folder.</p>"},{"location":"home/about/","title":"About us","text":"<ul> <li>Gy\u0151r, Hungary, Europe</li> <li>Sz\u00e9chenyi University IS Building 2nd floor</li> <li>+36 96 613 680</li> <li>jkk@sze.hu</li> <li>jkk.sze.hu</li> <li>youtube.com/jkk-sze-research</li> </ul>"},{"location":"home/social/","title":"Social media","text":"<ul> <li>youtube.com/jkk-sze-research</li> <li>instagram.com/jkk.sze</li> <li>youtube.com/szenergyteam</li> <li>instagram.com/szenergyteam</li> </ul>"},{"location":"home/videos/","title":"Videos","text":""},{"location":"papers/","title":"Papers overview","text":"<ul> <li>Curve Trajectory Model for Human Preferred Path Planning of Automated Vehicles - Gerg\u0151 Ign\u00e9czi, Ern\u0151 Horv\u00e1th, Roland T\u00f3th, Krisztian Nyilas  - Springer Automotive Innovation PDF <code>2024</code> </li> <li>Deep Learning-Based Approach for Autonomous Vehicle Localization: Application and Experimental Analysis - Norbert Mark\u00f3, Ern\u0151 Horv\u00e1th, Istv\u00e1n Szalay, Kriszti\u00e1n Enisz -  Machines, vol. 11, no. 12, p. 1079, <code>2023</code> </li> <li>Network Optimization Aspects of Autonomous Vehicles: Challenges and Future Directions - Rudolf Krecht, Tam\u00e1s Budai, Ern\u0151 Horv\u00e1th, \u00c1kos Kov\u00e1cs, Nobert Mark\u00f3, Mikl\u00f3s Unger  - IEEE Network <code>2023</code></li> <li>Node Point Optimization for Local Trajectory Planners based on Human Preferences - Gerg\u0151 Ign\u00e9czi, Ern\u0151 Horv\u00e1th  - 21st World Symposium on Applied Machine Intelligence and Informatics (SAMI) <code>2023</code> </li> <li>Real-Time LIDAR-Based Urban Road and Sidewalk Detection for Autonomous Vehicles - Ern\u0151 Horv\u00e1th,Claudiu Radu Pozna, Mikl\u00f3s Unger -  Sensors, vol. 22, no. 1, p. 194, <code>2022</code> </li> <li>A Clothoid-based Local Trajectory Planner with Extended Kalman Filter - Gerg\u0151 Ign\u00e9czi, Ern\u0151 Horv\u00e1th  - IEEE 20th Jubilee World Symposium on Applied Machine Intelligence and Informatics (SAMI) <code>2022</code>, Poprad, Slovakia</li> <li>Hybrid Particle Filter-Particle Swarm Optimization Algorithm and Application to Fuzzy Controlled Servo Systems - Claudiu Pozna, Radu-Emil Precup, Ern\u0151 Horv\u00e1th, Emil M. Petriu - IEEE Transactions on Fuzzy Systems, <code>2022</code></li> <li>Implementation of a self-developed Model Predictive Control Scheme for Vehicle Parking Maneuvers - Gerg\u0151 Ign\u00e9czi, Ern\u0151 Horv\u00e1th, D\u00e1niel Pup  - The 1st ISTRC Annual Conference, PDF <code>2021</code>, Tel Aviv, Israel</li> <li>Case Study on the Tactical Level of an Autonomous Vehicle Control - Claudiu Radu Pozna, Csaba Antonya, Ern\u0151 Horv\u00e1th  - International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME), <code>2021</code>, Mauritius, Mauritius</li> <li>Clothoid-based Trajectory Following Approach for Self-driving vehicles - Ern\u0151 Horv\u00e1th, Claudiu Radu Pozna - IEEE 19th World Symposium on Applied Machine Intelligence and Informatics (SAMI), <code>2021</code>, Herl'any, Slovakia, Virtual</li> <li>Development of Point-cloud Processing Algorithm for Self-Driving Challenges - Mikl\u00f3s Unger, Ern\u0151 Horv\u00e1th, P\u00e9ter K\u0151r\u00f6s - IEEE International Conference on Intelligent Engineering Systems (INES), <code>2020</code>, Reykjav\u00edk, Iceland, Virtual</li> <li>Self-Driving Vehicle Sensors from One-Seated Experimental to Road-legal Vehicle - P\u00e9ter K\u0151r\u00f6s, G\u00e1bor Szak\u00e1llas, P\u00e9ter Guly\u00e1s, Zolt\u00e1n Pusztai, Zolt\u00e1n Szeli, Ern\u0151 Horv\u00e1th - IEEE International Conference on Intelligent Engineering Systems (INES), <code>2020</code>, Reykjav\u00edk, Iceland, Virtual</li> <li>Improving the efficiency of neural networks with virtual training data - J\u00e1nos Holl\u00f3si, Rudolf Krecht, Norbert Mark\u00f3, \u00c1ron Ballagi - Hungarian Journal of Industry and Chemistry, Self-Driving Vehicles Special Issue, PDF <code>2020</code>, Hungary</li> <li>Theoretical background and application of multiple goal pursuit trajectory follower - Ern\u0151 Horv\u00e1th, Claudiu Radu Pozna, P\u00e9ter K\u0151r\u00f6s, Csaba Hajdu, \u00c1ron Ballagi - Hungarian Journal of Industry and Chemistry, Self-Driving Vehicles Special Issue, PDF <code>2020</code>, Hungary  </li> <li>LIDAR-based Collision-Free Space Estimation Approach - Mikl\u00f3s Unger, Ern\u0151 Horv\u00e1th, Csaba Hajdu - Hungarian Journal of Industry and Chemistry, Self-Driving Vehicles Special Issue, PDF <code>2020</code>, Hungary</li> <li>Towards System-Level Testing with Coverage Guarantees for Autonomous Vehicles - Istv\u00e1n Majzik, Oszk\u00e1r Semer\u00e1th, Csaba Hajdu, Krist\u00f3f Marussy, Zolt\u00e1n Szatm\u00e1ri, Zolt\u00e1n Micskei, Andr\u00e1s V\u00f6r\u00f6s, Aren A. Babikian, D\u00e1niel Varr\u00f3 - ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems (MODELS) <code>2019</code>, Munich, Germany</li> <li>Range Sensor-based Occupancy Grid Mapping with Signatures - Ern\u0151 Horv\u00e1th, Csaba Hajdu, Claudiu Radu Pozna, \u00c1ron Ballagi - 20th International Carpathian Control Conference (ICCC), <code>2019</code>, Krakow-Wieliczka, Poland</li> <li>Novel Pure-Pursuit Trajectory Following Approaches and their Practical Applications - Ern\u0151 Horv\u00e1th, Csaba Hajdu and Peter K\u0151r\u00f6s - 10th IEEE International Conference on InfoCommunications <code>2019</code>, Naples, Italy</li> <li>Improve the Accuracy of Neural Networks using Capsule Layers - J\u00e1nos Holl\u00f3si, Claudiu Radu Pozna - IEEE 18th International Symposium on Computational Intelligence and Informatics (CINTI), <code>2018</code>, Budapest, Hungary</li> </ul>"},{"location":"workshops/","title":"Workshops overview","text":""},{"location":"workshops/#20240608-gyor-ros-2","title":"<code>2024.06.08</code> Gy\u0151r <code>ROS 2</code>","text":"<p>During the workshop F1/10 based simulation was presented based on <code>ROS 2</code> Humble.</p> <p> </p>"},{"location":"workshops/#20240418-kempten-ros-2","title":"<code>2024.04.18</code> Kempten <code>ROS 2</code>","text":"<p>During the workshop F1/10 hands-on session was presented based on <code>ROS 2</code> Humble.</p> <p> </p>"},{"location":"workshops/#20231103-gyor-ros-1","title":"<code>2023.11.03</code> Gy\u0151r <code>ROS 1</code>","text":"<p>During the workshop a very simple wall following approach was presented with hands-on experinece on a real robot. </p> <p> </p>"},{"location":"workshops/clustering_a/","title":"Workshop on ROS 2 LIDAR clustering","text":"<p>This short workshop will guide you how to filter and cluster LIDAR data into objets. Objects in our case are pedestrians, cars, buildings and so on. The workshop is ROS 2 compatible </p>"},{"location":"workshops/clustering_a/#requirements-high-level-overview","title":"Requirements (high-level overview)","text":"<ol> <li>ROS 2 Humble: \ud83d\udfe0 see previous workshops or docs.ros.org/en/humble/Installation.html </li> <li>A log file with raw LIDAR data (MCAP format) \u2705 </li> <li>Filtering out the ground plane with <code>patchworkpp</code> \u2705 </li> <li>Clustering the non-grund data into objects with <code>lidar_cluster</code> \u2705 </li> </ol>"},{"location":"workshops/clustering_a/#video-owerview","title":"Video owerview","text":"<p>In the following a screen record of the steps are presented: TODO:</p>"},{"location":"workshops/clustering_a/#step-1-download-the-raw-data","title":"<code>Step 1.</code> - Download the raw data","text":"<p>In order to cluster LIDAR data, first you need - no surprise - LIDAR data. Use any of the following 3 option.</p>"},{"location":"workshops/clustering_a/#option-a-download-our-mcap-from-a-link","title":"<code>Option A.</code> - Download our MCAP from a link","text":"<p>Download MCAP [~540MB]  </p> <p>In our case <code>/mnt/c/bag/</code> is used as the <code>.mcap</code> folder, if you use other, modify it when executing the following steps.</p>"},{"location":"workshops/clustering_a/#option-b-download-our-mcap-as-a-terminal-command","title":"<code>Option B.</code> - Download our MCAP as a terminal command","text":"Don't forget to change directory first.  In our case /mnt/c/bag/ is used as a final destination:  <pre><code>cd /mnt/c/bag/\n</code></pre> <pre><code>wget https://laesze-my.sharepoint.com/:u:/g/personal/herno_o365_sze_hu/Eclwzn42FS9GunGay5LPq-EBA6U1dZseBFNDrr6P0MwB2w?download=1  -O lexus3-2024-04-05-gyor.mcap\n</code></pre>"},{"location":"workshops/clustering_a/#option-c-use-own-mcap","title":"<code>Option C.</code> - Use own MCAP","text":"<p>You can use your own MCAP, please note that you have to change following:</p> <ul> <li>LIDAR topic which is on our case <code>/lexus3/os_center/points</code> and the </li> <li>LIDAR frame (in our case: <code>lexus3/os_center_a_laser_data_frame</code>) </li> </ul> <p>In the following steps based on your own data, topic and frame must be set.</p>"},{"location":"workshops/clustering_a/#check-your-raw-data","title":"Check your raw data","text":"<p>Play your bag e.g. with: <pre><code>ros2 bag play /mnt/c/bag/lexus3-2024-04-05-gyor.mcap -l\n</code></pre></p> <p>Info</p> <p>In the <code>play</code> command <code>-l</code> means to loop the mcap file.</p> <p>Success</p> <p>If everything works as expected you shold see a bunch of topics in another terminal   Topics In another terminal issue the command: <p><pre><code>ros2 topic list\n</code></pre> You sholud see a similar list opf topics:</p> <p><pre><code>/clock\n/events/read_split\n/lexus3/gps/duro/current_pose\n/lexus3/gps/duro/imu\n/lexus3/gps/duro/mag\n/lexus3/gps/duro/navsatfix\n/lexus3/gps/duro/status_flag\n/lexus3/gps/duro/status_string\n/lexus3/gps/duro/time_diff\n/lexus3/gps/duro/time_ref\n/lexus3/os_center/points\n/lexus3/os_left/points\n/lexus3/os_right/points\n/lexus3/zed2i/zed_node/left/image_rect_color/compressed\n/parameter_events\n/rosout\n/tf\n/tf_static   \n</code></pre> </p> <p>Also there must be at least a <code>sensor_msgs/msg/PointCloud2</code>, chechk with: <pre><code> ros2 topic type /lexus3/os_center/points\n</code></pre> Result: <pre><code>sensor_msgs/msg/PointCloud2\n</code></pre></p>"},{"location":"workshops/clustering_a/#step-2-install-ros-2-packages","title":"<code>Step 2.</code> - Install <code>ROS 2</code> packages","text":"<p>Info</p> <p>If you don't have <code>~/ros2_ws/</code> directory, use your own workspace, or create it: <pre><code>mkdir -p ~/ros2_ws/src\n</code></pre></p>"},{"location":"workshops/clustering_a/#clone-patchworkpp-package","title":"Clone <code>patchworkpp</code> package","text":"<p><code>patchwork-plusplus-ros</code> is ROS 2 package of Patchwork++ (@ IROS'22), which provides fast and robust LIDAR ground segmentation. This package is developed by KAIST (Korea Advanced Institute of Science &amp; Technology), but you can use the JKK research fork too.</p> <pre><code>cd ~/ros2_ws/src\n</code></pre> <pre><code>git clone https://github.com/jkk-research/patchwork-plusplus-ros\n</code></pre> <p>Alternatively you can download the <code>ROS2</code> branch from KAIST:</p> <pre><code>git clone https://github.com/url-kaist/patchwork-plusplus-ros -b ROS2\n</code></pre>"},{"location":"workshops/clustering_a/#clone-lidar_cluster-package","title":"Clone <code>lidar_cluster</code> package","text":"<pre><code>cd ~/ros2_ws/src\n</code></pre> <pre><code>git clone https://github.com/jkk-research/lidar_cluster_ros2\n</code></pre>"},{"location":"workshops/clustering_a/#build-the-packages","title":"Build the packages","text":"<pre><code>cd ~/ros2_ws\n</code></pre> <pre><code>colcon build --packages-select patchworkpp lidar_cluster --symlink-install\n</code></pre>"},{"location":"workshops/clustering_a/#what-to-expect","title":"What to expect","text":"<pre><code>graph TD;\n\n    p1[ /lexus3/os_center/points&lt;br/&gt;sensor_msgs::PointCloud2]:::white --&gt; patchwork([ /patchwork_node]):::light\n    patchwork --&gt; p\n    p[ /nonground&lt;br/&gt;sensor_msgs::PointCloud2]:::white --&gt; cluster([ /cluster_node]):::light\n    cluster --&gt; f1[ /clustered_points&lt;br/&gt;sensor_msgs::PointCloud2]:::white\n    cluster --&gt; f2[ /clustered_marker&lt;br/&gt;visualization_msgs::MarkerArray]:::white\n    classDef light fill:#34aec5,stroke:#152742,stroke-width:2px,color:#152742  \n    classDef dark fill:#152742,stroke:#34aec5,stroke-width:2px,color:#34aec5\n    classDef white fill:#ffffff,stroke:#152742,stroke-width:2px,color:#15274\n    classDef dash fill:#ffffff,stroke:#152742,stroke-width:2px,color:#15274, stroke-dasharray: 5 5\n    classDef red fill:#ef4638,stroke:#152742,stroke-width:2px,color:#fff</code></pre>"},{"location":"workshops/clustering_a/#run","title":"Run","text":"Don't forget to source before ROS commands. <pre><code>source ~/ros2_ws/install/setup.bash\n</code></pre> <pre><code>ros2 bag play /mnt/c/bag/lexus3-2024-04-05-gyor.mcap -l\n</code></pre> <p><pre><code>ros2 launch patchworkpp demo.launch.py  cloud_topic:=/lexus3/os_center/points cloud_frame:=lexus3/os_center_a_laser_data_frame\n</code></pre> Use an example clustering algorithm (<code>version 1.</code>):</p> <pre><code>ros2 launch lidar_cluster dbscan_spatial.launch.py\n</code></pre> <p>Alternatively use an example clustering algorithm (<code>version 2.</code>): <pre><code>ros2 launch lidar_cluster euclidean_spatial.launch.py\n</code></pre></p> <p>Alternatively use an example clustering algorithm (<code>version 2.</code>): <pre><code>ros2 launch lidar_cluster euclidean_grid.launch.py\n</code></pre></p> <pre><code>ros2 launch lidar_cluster rviz02.launch.py\n</code></pre> <p>Success</p> <p>TODO: If everything works as expected you should see a similar rviz window. </p>"},{"location":"workshops/f1tenth_real_a/","title":"<code>ROS 2</code> F1/10 hands-on workshop","text":"<ul> <li>A small overview of the <code>Bavarian-Hungarian Self-driving vehicles</code> workshop.</li> <li>Date: 2024.06.08. </li> <li>Place: Gy\u0151r, Hungary.</li> </ul>"},{"location":"workshops/f1tenth_real_a/#a-ros-2-package","title":"A <code>ROS 2</code> package","text":""},{"location":"workshops/f1tenth_real_a/#hands-on","title":"Hands-on","text":"<p><code>ROS 2</code> humble </p>"},{"location":"workshops/f1tenth_sim_a/","title":"<code>ROS 2</code> F1/10 Wheeltec Gazebo simulation workshop","text":"<p>The workshop is ROS 2 compatible </p>"},{"location":"workshops/f1tenth_sim_a/#video","title":"Video","text":""},{"location":"workshops/f1tenth_sim_a/#requirements-high-level","title":"Requirements (high-level)","text":"<ol> <li>ROS 2 Humble: \ud83d\udfe0 see previous workshops or docs.ros.org/en/humble/Installation.html </li> <li>Gazebo Fortress: \u2705 current workshop gazebosim.org/docs/fortress/install_ubuntu</li> <li><code>ROS gz bridge</code>:  \u2705 current workshop, ROS integration. Install with a single command: <code>sudo apt install ros-humble-ros-gz-bridge</code>, gazebosim.org/docs/fortress/ros2_integration</li> <li>Build and run custom worlds and models  \u2705 current workshop (e.g. <code>F1/10</code> / <code>Wheeltec, Roboworks</code>) </li> </ol> Official F1/10 vehicle vs Wheeltec Roboworks Ackermann Rosbot mini vehicle"},{"location":"workshops/f1tenth_sim_a/#binary-installation-on-ubuntu","title":"Binary Installation on Ubuntu","text":"<p>Fortress binaries are provided for Ubuntu Bionic, Focal and Jammy. All of the Fortress binaries are hosted in the osrfoundation repository. To install all of them, the metapackage <code>ignition-fortress</code> can be installed. The following is based on gazebosim.org/docs/fortress/install_ubuntu.</p> <p>First install some necessary tools:</p> <p><pre><code>sudo apt-get update\n</code></pre> <pre><code>sudo apt-get install lsb-release wget gnupg\n</code></pre></p> <p>Then install Ignition Fortress:</p> <p><pre><code>sudo wget https://packages.osrfoundation.org/gazebo.gpg -O /usr/share/keyrings/pkgs-osrf-archive-keyring.gpg\n</code></pre> <pre><code>echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/pkgs-osrf-archive-keyring.gpg] http://packages.osrfoundation.org/gazebo/ubuntu-stable $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/gazebo-stable.list &gt; /dev/null\n</code></pre> <pre><code>sudo apt-get update\n</code></pre> <pre><code>sudo apt-get install ignition-fortress\n</code></pre></p> <p>All libraries should be ready to use and the <code>ign gazebo</code> app ready to be executed.</p>"},{"location":"workshops/f1tenth_sim_a/#gazebo-fortress-ros-2-integration","title":"Gazebo Fortress ROS 2 integration","text":"<p>Issue the following command:</p> <pre><code>sudo apt install ros-humble-ros-gz-bridge\n</code></pre>"},{"location":"workshops/f1tenth_sim_a/#additional-settings-to-wsl2","title":"Additional settings to WSL2","text":"<p>Warning - WSL2</p> <p>There is an issue, which can be set even in <code>~/.bashrc</code>:</p> <pre><code>export LIBGL_ALWAYS_SOFTWARE=1\n</code></pre> <p>Set it in <code>~/.bashrc</code>: <pre><code>echo \"export LIBGL_ALWAYS_SOFTWARE=1\" &gt;&gt; ~/.bashrc\n</code></pre></p>  Don't forget to source bashrc. <pre><code>source ~/.bashrc\n</code></pre> <p>After new terminal or <code>source</code>:</p> <pre><code>echo $LIBGL_ALWAYS_SOFTWARE\n</code></pre> <p>should  print <code>1</code>. Alternatively </p> <p><pre><code>cat ~/.bashrc | grep LIBGL\n</code></pre> should print the line.</p>"},{"location":"workshops/f1tenth_sim_a/#check-the-installation","title":"Check the installation","text":"<p>Success</p> <p>Now the <code>ign gazebo</code> should work and the <code>ros2</code> commands should be available.</p> <p></p> <p>Try at least one of the following commands:</p> <pre><code>ign gazebo\n</code></pre> <pre><code>ign gazebo -v 4 -r ackermann_steering.sdf\n</code></pre> <pre><code>ign gazebo shapes.sdf\n</code></pre> <p></p> <pre><code>ign param --versions\n</code></pre>"},{"location":"workshops/f1tenth_sim_a/#packages-and-build","title":"Packages and build","text":"<p>Detailed description of the packages and build process.</p> <p>It is assumed that the workspace is <code>~/ros2_ws/</code>.</p> <pre><code>cd ~/ros2_ws/src\n</code></pre> <pre><code>git clone https://github.com/rudolfkrecht/robotverseny\n</code></pre>"},{"location":"workshops/f1tenth_sim_a/#build","title":"Build","text":"<pre><code>cd ~/ros2_ws\n</code></pre> <pre><code>colcon build --symlink-install --packages-select robotverseny_application robotverseny_description robotverseny_bringup robotverseny_gazebo \n</code></pre>"},{"location":"workshops/f1tenth_sim_a/#run","title":"Run","text":"Don't forget to source before ROS commands. <pre><code>source ~/ros2_ws/install/setup.bash\n</code></pre> <pre><code>ros2 launch robotverseny_bringup roboworks.launch.py\n</code></pre>"},{"location":"workshops/f1tenth_sim_a/#useful-commands","title":"Useful commands","text":"<p>Publish command topic: <pre><code>ros2 topic pub --once /roboworks/cmd_vel geometry_msgs/msg/Twist \"{linear: {x: 2.5, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: -0.01}}\"\n</code></pre></p> <p>Teleop twist keyboard: <pre><code>ros2 run teleop_twist_keyboard teleop_twist_keyboard --ros-args -r /cmd_vel:=/roboworks/cmd_vel\n</code></pre></p> <p>Ignition info topic: <pre><code>ign topic -i --topic /model/roboworks/cmd_vel\n</code></pre> Ignition echo topic:</p> <pre><code>ign topic -et /model/roboworks/cmd_vel\n</code></pre> <p>Topics:</p> <pre><code>ros2 topic list\n</code></pre>  Here are the topics. <pre><code>/clicked_point\n/clock\n/goal_pose\n/initialpose\n/joint_states\n/parameter_events\n/robot_description\n/roboworks/cmd_vel\n/roboworks/odometry\n/roboworks/scan\n/rosout\n/tf\n/tf_static\n</code></pre>"},{"location":"workshops/ros2_a/","title":"<code>ROS 2</code> hands-on workshop","text":"<ul> <li>A small overview of the <code>Bavarian-Hungarian Self-driving vehicles</code> workshop.</li> <li>Date: 2024.06.08. </li> <li>Place: Gy\u0151r, Hungary.</li> </ul>"},{"location":"workshops/ros2_a/#a-ros-2-package","title":"A <code>ROS 2</code> package","text":""},{"location":"workshops/wheeltec_real_a/","title":"<code>ROS 1</code> real robot workshop","text":"<ul> <li>A small overview of the <code>Bavarian-Hungarian Self-driving vehicles</code> workshop.</li> <li>Date: 2023.11.03. </li> <li>Place: Gy\u0151r, Hungary.</li> </ul>"},{"location":"workshops/wheeltec_real_a/#the-megoldas_zala23-ros-1-package","title":"The <code>megoldas_zala23</code> ROS 1 package","text":"<p>\ud83e\udd16 In the following a very simple wall/gap following approach will be presented and described. The origin of he code is based on the work of Suresh Babu (University of Virginia, license). Link to the original code: github.com/linklab-uva/f1tenth_gtc_tutorial.</p> <p>The name of the package is a comes from a hungarian expression (<code>megoldas</code>: solution / L\u00f6sung).</p>"},{"location":"workshops/wheeltec_real_a/#the-robot-used-in-the-competition","title":"The robot used in the competition","text":"<p>Wheeltec / Roboworks Rosbot mini Ackermann robot </p> <p>On-board computer - Nvidia Jetson Nano</p> <p>Sensors - Orbbec Depth Camera - LSN10 LIDAR</p>"},{"location":"workshops/wheeltec_real_a/#video","title":"Video","text":""},{"location":"workshops/wheeltec_real_a/#usage","title":"Usage","text":"<p>Prerequisites: - WiFi-enabled computer with Ubuntu 18.04 / 20.04 operating system and ROS Melodic / Noetic installation - Internet access (Ethernet cable or WiFi)</p> <ol> <li>Turn on the robot platform.</li> <li>Use the computer to connect to the WiFi network created by the robot. The name of the WiFi network is unique for each robot platform, the <code>#</code> at the end of the SSID changes according to the number of the robot platform: <pre><code>SSID: WHEELTEC_CAR_5.5_#\nPassword: dongguan\n</code></pre></li> <li>Use SSH to connect to the on-board computer of the robot platform with the following terminal command: <pre><code>ssh wheeltec@192.168.0.100\n</code></pre> A password will be required, the default password is <code>dongguan</code></li> </ol>"},{"location":"workshops/wheeltec_real_a/#internet-access-on-the-robot-platform","title":"Internet access on the robot platform","text":"<p>Software packages can be downloaded to the on-board computer of the robot platform, which requires internet access.</p> <ul> <li>Ethernet: connect the Ethernet cable to the Ethernet port of the on-board computer of the robot platform.</li> <li>WiFi: after issuing the <code>nmtui</code> terminal command, connect to the available WiFi network. <pre><code>nmtui\n</code></pre></li> </ul> <p></p>"},{"location":"workshops/wheeltec_real_a/#install-the-ros-1-package","title":"Install the <code>ROS 1</code> package","text":"<p>After installation, the functions of the robot platform can be accessed using ROS. The sample solution of the competition can also be deployed by ROS.</p> <p>Create a workspace and install the sample solution on the robot:</p> <pre><code>mkdir -p ~/workshop_ws/src\n</code></pre> <pre><code>cd ~/workshop_ws/\n</code></pre> <pre><code>catkin init\n</code></pre> <pre><code>cd ~/workshop_ws/src/\n</code></pre> <pre><code>git clone https://github.com/robotverseny/megoldas_zala23\n</code></pre> <pre><code>cd ~/workshop_ws/\n</code></pre> <pre><code>catkin build megoldas_zala23\n</code></pre> <pre><code>echo \"source /home/wheeltec/workshop_ws/devel/setup.bash\" &gt;&gt; ~/.bashrc\n</code></pre> <pre><code>source ~/.bashrc\n</code></pre> <p>Install <code>screen</code> <pre><code>sudo apt install mc screen\n</code></pre></p> <p>Install jks visualization rviz plugin: depending on ROS 1 version (melodic / noetic):</p> <pre><code>sudo apt install ros-melodic-jsk-rviz-plugins\n</code></pre> <pre><code>sudo apt install ros-noetic-jsk-rviz-plugins\n</code></pre>"},{"location":"workshops/wheeltec_real_a/#usage_1","title":"Usage","text":""},{"location":"workshops/wheeltec_real_a/#start-solution-using-screen-recommended","title":"Start solution using screen (recommended)","text":"<p>The script <code>verseny_start.sh</code> sets the required environmental variables, starts the ROS nodes and finally after 2 minutes stops everything. Have a look at the code: verseny_start.sh </p> <pre><code>rosrun megoldas_zala23 verseny_start.sh\n</code></pre> <p>The <code>verseny_start.sh</code> shell script usually launches several virtual terminals, such as: <code>roscore</code>, <code>turn_on_wheeltec_robot</code>, <code>lsn10_lidar</code>, <code>megoldas1.launch</code>. All components of the solution can be stopped with the following command: <pre><code>rosrun megoldas_zala23 stop_all.sh\n</code></pre></p> <p>Further commands:</p> <ul> <li>list screen: <code>screen -ls</code></li> <li>restore screen: <code>screen -r roscore</code> / <code>screen -r turn_on_wheeltec_robot</code> / <code>screen -r megoldas1</code></li> <li>detach: <code>Ctrl-a</code> + <code>Ctrl-d</code></li> </ul>"},{"location":"workshops/wheeltec_real_a/#ros-connection","title":"ROS connection","text":"<p>The ROS topics advertised by the robot platform are also available on the computer connected to the platform, with the appropriate setting of the <code>ROS_MASTER_URI</code> variable: <pre><code>export ROS_MASTER_URI=http://192.168.0.100:11311\n</code></pre> After the appropriate setting of the variable, the topics can be listed and visualized using Rviz: <pre><code>rostopic list\n</code></pre> <pre><code>rosrun rviz rviz\n</code></pre></p>"},{"location":"workshops/wheeltec_real_a/#some-explanatory-animations","title":"Some explanatory animations","text":"<pre><code>roslaunch megoldas_zala23 rviz1.launch\n</code></pre>"},{"location":"workshops/wheeltec_real_a/#start-solution-per-component","title":"Start solution per component","text":"<p>The solution can also be started per component, not just as a single shell script. This requires four terminal windows on the on-board computer of the robot platform and issuing the following commands per terminal:</p> <pre><code>roscore\n</code></pre> <pre><code>roslaunch turn_on_wheeltec_robot turn_on_wheeltec_robot.launch\n</code></pre> <pre><code>roslaunch lsn10 lsn10.launch\n</code></pre> <pre><code>roslaunch megoldas_zala23 megoldas1.launch\n</code></pre>"},{"location":"workshops/wheeltec_real_a/#additional-information","title":"Additional information","text":""},{"location":"workshops/wheeltec_real_a/#workspaces","title":"Workspaces","text":"<pre><code>~/wheeltec_robot/src\n~/catkin_workspace/src\n~/workshop_ws/src/\n</code></pre>"},{"location":"workshops/wheeltec_real_a/#topic-management","title":"Topic management","text":"<pre><code>rostopic hz /scan\nrostopic echo /scan -n1\nrostopic type /scan\n</code></pre> <pre><code>sensor_msgs/LaserScan\n</code></pre>"},{"location":"workshops/wheeltec_real_a/#robot-platform-language-settings","title":"Robot platform language settings","text":"<pre><code>sudo dpkg-reconfigure locales\n</code></pre> <p>reboot</p>"},{"location":"workshops/wheeltec_real_a/#rosbag-management","title":"Rosbag management","text":"<p><pre><code>cd ~/rosbags\nrosbag record -a -o test1\n</code></pre> <pre><code>rsync -avzh --progress wheeltec@192.168.0.100:/home/wheeltec/rosbags/ /mnt/c/bag/wheeltec/\nrosbag info test1_2023-03-30-12-37-22.bag\nrosbag play test1_2023-03-30-12-37-22.bag\n</code></pre></p> <p>You can even visualize rosbags in Foxglove studio:</p> <p></p> <p>Download rosbags</p> <ul> <li>Further explanation ipynotebook</li> <li>Competition homepage</li> <li>Foxglove studio</li> </ul>"}]}